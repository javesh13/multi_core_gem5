{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"embedding_gen.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"cells":[{"cell_type":"code","metadata":{"id":"833ab27d","scrolled":true,"outputId":"e166f763-8e4e-4426-c316-3d0e9111c39f"},"source":["from sklearn.cluster import KMeans\n","import pandas as pd, numpy as np\n","from matplotlib import pyplot as plt\n","from collections import Counter\n","from sklearn.metrics import silhouette_score\n","import math\n","\n","from keras import Model\n","from keras.layers import LSTM, Input, Dense, Concatenate, Embedding, Dropout\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.optimizers import SGD\n","from keras.models import load_model\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OrdinalEncoder\n","print(tf.config.list_physical_devices('GPU'))\n"],"id":"833ab27d","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"c56be88d"},"source":["## Constants\n"],"id":"c56be88d"},{"cell_type":"code","metadata":{"id":"29bb289f"},"source":["batch_size = 100\n","debug  =0"],"id":"29bb289f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b001c825","scrolled":true,"outputId":"6dd948c6-5826-4835-f659-5485507d9bb5"},"source":["\n","# train=600000\n","df_for_training = pd.read_csv('result_ii.txt', usecols=[1, 2],nrows=1000000)#reading only PC and Delta\n","cols = df_for_training.columns\n","df_for_training = df_for_training[cols].astype(float)\n","cols = df_for_training.columns\n","\n","print(df_for_training.shape)\n","\n","vocab_deltas = pd.read_csv(\"vocab_deltas.txt\", dtype=np.int32,nrows=2000)\n","vocab_deltas = vocab_deltas['top_deltas'].tolist()\n","\n","print(\"vocab_deltas is of type\",type(vocab_deltas[0]))\n","print(\"Vocab size is: \", len(vocab_deltas))\n","vocab_size = len(vocab_deltas)\n","\n","MAX_DELTA = int(df_for_training['Delta'].nunique()) + 1\n","MAX_PC = int(df_for_training['PC'].nunique()) + 1\n","\n","\n","oe_PC = OrdinalEncoder()\n","oe_Delta = OrdinalEncoder()\n","\n","oe_PC = oe_PC.fit(df_for_training['PC'].values.reshape(-1, 1))\n","oe_Delta = oe_Delta.fit(df_for_training['Delta'].values.reshape(-1, 1))\n","\n","\n","from sklearn.preprocessing import OneHotEncoder\n","ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n","ohe.fit(np.array(vocab_deltas).reshape(-1,1))"],"id":"b001c825","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(1000000, 2)\n","vocab_deltas is of type <class 'int'>\n","Vocab size is:  500\n"]},{"data":{"text/plain":["OneHotEncoder(handle_unknown='ignore', sparse=False)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"a21cd848"},"source":["from tensorflow.python.keras.utils import data_utils\n","\n","\n","class CustomDataGen(data_utils.Sequence):\n","    \n","    def __init__(self, df, batch_size, ohe, oe_pc, oe_delta):\n","        self.df = df.copy()\n","        self.n_future = 1\n","        self.n_past = 20\n","        self.batch_size = batch_size\n","        self.n = len(self.df) - self.n_past  ##calculate it precisely\n","        self.n = math.ceil(self.n // self.batch_size)\n","        self.n -=1\n","        self.ohe = ohe\n","        self.ix = 0\n","        self.ohe = ohe\n","        self.tmp_y = self.df['Delta'].copy().to_numpy()\n","        self.oe_pc = oe_pc\n","        self.oe_delta = oe_delta\n","\n","        self.df['PC'] = self.oe_pc.transform(self.df['PC'].values.reshape(-1, 1))\n","        self.df['Delta'] = self.oe_delta.transform(self.df['Delta'].values.reshape(-1, 1))\n","        # print(self.df.head())\n","        self.df = self.df.to_numpy(dtype=np.float64)\n","        self.iterator_index = 0\n","        # print(self.df[0])\n","\n","    def __next__(self):\n","        data = self.__getitem__(self.ix)\n","        self.ix  += 1\n","        self.ix %= self.n\n","        return data\n","    \n","    def __iter__(self):\n","        while True:\n","            data= self.__getitem__(self.iterator_index)\n","            self.iterator_index += 1\n","            self.iterator_index %= self.n\n","            yield data\n","    def reset_iterator(self):\n","        self.iterator_index = 0\n","    \n","    def __getitem__(self, index):\n","        global debug\n","        \"\"\"\n","        index is batch_index\n","        each batch will have batch_size elements\n","        \"\"\"\n","        index %= self.n\n","        X = []\n","        Y = []\n","        start = index * self.batch_size\n","        for i in range(start, start+batch_size):\n","            X.append(self.df[i:i + self.n_past, :])\n","            Y.append(self.ohe.transform(np.array(self.tmp_y[i + self.n_past : i + self.n_past + 1]).\n","                                        reshape(-1,1)))\n","            \n","        X = np.array(X)\n","        Y = np.array(Y)\n","        shp = Y.shape\n","        Y = Y.reshape(shp[0], shp[2])\n","        \n","        \n","        tmp = np.split(X, 2, axis=2)\n","        tmp[0] = tmp[0].reshape((batch_size, 20))\n","        tmp[1] = tmp[1].reshape((batch_size, 20))\n","        if (debug == 1 and index == 0):\n","          print(tmp, y)\n","        return (tmp, Y)\n","\n","    def getdatapoint(self, i):\n","        return self.df[i:i+self.n_past, :]\n","    \n","    def __len__(self):\n","        return self.n"],"id":"a21cd848","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40adcbcf","outputId":"1927220a-6f58-4ecc-e3ff-5a22c75c7543"},"source":["print(df_for_training.shape)"],"id":"40adcbcf","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(1000000, 2)\n"]}]},{"cell_type":"code","metadata":{"id":"010069eb"},"source":["\n","df_train, df_test = train_test_split(df_for_training, test_size = 0.25, shuffle=False)\n","df_train, df_validate = train_test_split(df_train, test_size = 0.20, shuffle=False)\n","\n","# df_test['PC'] = oe_PC.transform(df_test['PC'].values.reshape(-1, 1))\n","\n","# df_all = CustomDataGen(df_for_training, batch_size, ohe, oe_PC, oe_Delta)\n","data_gen_train = CustomDataGen(df_train, batch_size, ohe, oe_PC, oe_Delta)\n","data_gen_validate = CustomDataGen(df_validate, batch_size, ohe, oe_PC, oe_Delta)\n","data_gen_test = CustomDataGen(df_test, batch_size, ohe, oe_PC, oe_Delta)\n","\n"],"id":"010069eb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bd231a1"},"source":["\n","\n"],"id":"6bd231a1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"code_folding":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"7e1cc154","scrolled":true,"outputId":"37cc31f6-1d4f-4abb-edf2-71a89cae1431"},"source":["EMBEDDING_SIZE = 10\n","n_past=20\n","PC_INP = Input(shape=(n_past,), name=\"input_1\")\n","PC_EMB = Embedding(MAX_PC+1, EMBEDDING_SIZE, input_length=n_past)(PC_INP)\n","\n","DELTA_INP = Input(shape=(n_past,), name=\"input_2\")\n","DELTA_EMB = Embedding(MAX_DELTA+1, EMBEDDING_SIZE,input_length=n_past)(DELTA_INP)\n","\n","\n","CONCATENATED_EMB = Concatenate(axis=1)([PC_EMB, DELTA_EMB])\n","\n","LSTM_1 = LSTM(128, activation = 'sigmoid', return_sequences=True)(CONCATENATED_EMB)\n","LSTM_2 = LSTM(128, activation='sigmoid', return_sequences=False)(LSTM_1)\n","OP_LAYER = Dense(vocab_size, activation='sigmoid')(LSTM_2)\n","\n","model = Model(inputs=[PC_INP, DELTA_INP], outputs=[OP_LAYER])\n","from keras import optimizers\n","optm = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","\n"],"id":"7e1cc154","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20)]         0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 20)]         0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 20, 10)       10860       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 20, 10)       24960       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 40, 10)       0           embedding[0][0]                  \n","                                                                 embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     (None, 40, 128)      71168       concatenate[0][0]                \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 128)          131584      lstm[0][0]                       \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 500)          64500       lstm_1[0][0]                     \n","==================================================================================================\n","Total params: 303,072\n","Trainable params: 303,072\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"b1783cbc","scrolled":true},"source":["metric=\"val_loss\"\n","es = EarlyStopping(monitor=metric, mode='min', verbose=1, patience=3)\n","mc = ModelCheckpoint('best_model.h5', verbose=1, save_best_only=True)"],"id":"b1783cbc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"e03fccd3","scrolled":true,"outputId":"51e01536-c705-4b16-f8e0-a41eee9643cd"},"source":["with tf.device(\"/GPU:3\"):\n","    history = model.fit(data_gen_train, steps_per_epoch = data_gen_train.__len__(),\n","                    validation_data=data_gen_validate ,validation_steps=data_gen_validate.__len__(),\n","                    epochs=30, batch_size=batch_size,callbacks=[es, mc],shuffle=False)\n","\n"],"id":"e03fccd3","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","5998/5998 [==============================] - 655s 109ms/step - loss: 4.0677 - accuracy: 0.1772 - val_loss: 2.3303 - val_accuracy: 0.3620\n","\n","Epoch 00001: val_loss improved from inf to 2.33031, saving model to best_model.h5\n","Epoch 2/20\n","5998/5998 [==============================] - 653s 109ms/step - loss: 2.8724 - accuracy: 0.3016 - val_loss: 2.0708 - val_accuracy: 0.4428\n","\n","Epoch 00002: val_loss improved from 2.33031 to 2.07081, saving model to best_model.h5\n","Epoch 3/20\n","5998/5998 [==============================] - 651s 109ms/step - loss: 3.1449 - accuracy: 0.2852 - val_loss: 2.2729 - val_accuracy: 0.3852\n","\n","Epoch 00003: val_loss did not improve from 2.07081\n","Epoch 4/20\n","5998/5998 [==============================] - 691s 115ms/step - loss: 2.3322 - accuracy: 0.3783 - val_loss: 1.8646 - val_accuracy: 0.4755\n","\n","Epoch 00004: val_loss improved from 2.07081 to 1.86464, saving model to best_model.h5\n","Epoch 5/20\n","5998/5998 [==============================] - 691s 115ms/step - loss: 1.9579 - accuracy: 0.4544 - val_loss: 1.6127 - val_accuracy: 0.5420\n","\n","Epoch 00005: val_loss improved from 1.86464 to 1.61268, saving model to best_model.h5\n","Epoch 6/20\n","5998/5998 [==============================] - 694s 116ms/step - loss: 1.7424 - accuracy: 0.5168 - val_loss: 1.4528 - val_accuracy: 0.5930\n","\n","Epoch 00006: val_loss improved from 1.61268 to 1.45276, saving model to best_model.h5\n","Epoch 7/20\n","5998/5998 [==============================] - 685s 114ms/step - loss: 1.5966 - accuracy: 0.5598 - val_loss: 1.3211 - val_accuracy: 0.6343\n","\n","Epoch 00007: val_loss improved from 1.45276 to 1.32113, saving model to best_model.h5\n","Epoch 8/20\n","5998/5998 [==============================] - 675s 113ms/step - loss: 1.4495 - accuracy: 0.5908 - val_loss: 1.2151 - val_accuracy: 0.6611\n","\n","Epoch 00008: val_loss improved from 1.32113 to 1.21507, saving model to best_model.h5\n","Epoch 9/20\n","5998/5998 [==============================] - 684s 114ms/step - loss: 1.3535 - accuracy: 0.6281 - val_loss: 1.0930 - val_accuracy: 0.6980\n","\n","Epoch 00009: val_loss improved from 1.21507 to 1.09299, saving model to best_model.h5\n","Epoch 10/20\n","5998/5998 [==============================] - 652s 109ms/step - loss: 1.2212 - accuracy: 0.6662 - val_loss: 0.9904 - val_accuracy: 0.7210\n","\n","Epoch 00010: val_loss improved from 1.09299 to 0.99036, saving model to best_model.h5\n","Epoch 11/20\n","4189/5998 [===================>..........] - ETA: 3:10 - loss: 1.1818 - accuracy: 0.6760"]}]},{"cell_type":"code","metadata":{"id":"4a12bdf9","scrolled":true},"source":["\n","\n","plt.plot(history.history['loss'], label='Training loss')\n","plt.plot(history.history['val_loss'], label='Validation loss')\n","plt.legend()"],"id":"4a12bdf9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd379d96","scrolled":true,"outputId":"4e62e50d-2063-489b-ce09-74e9414f67a6"},"source":["\n","saved_model = load_model('best_model.h5')\n","data_gen_test.reset_iterator()\n","Y_predict = saved_model.predict(data_gen_test, batch_size = batch_size,\n","                                steps=data_gen_test.__len__(),  verbose=1)\n","# data_gen_test.reset_iterator()\n","train_loss = saved_model.evaluate(data_gen_test, batch_size = batch_size,\n","                                      steps=data_gen_test.__len__(),  verbose=1)\n","print(train_loss)\n","print(Y_predict.shape)\n","print(type(Y_predict))\n","print(Y_predict.dtype)"],"id":"fd379d96","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","2498/2498 [==============================] - 97s 39ms/step\n","2498/2498 [==============================] - 99s 39ms/step - loss: 0.9412 - accuracy: 0.7287\n","[0.9411671161651611, 0.7286709547042847]\n","(249800, 500)\n","<class 'numpy.ndarray'>\n","float32\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"267e8673","outputId":"3a4908b3-dc3d-4d2e-c061-9944bb677767"},"source":["sub_total = []\n","print( data_gen_test.n)\n","for j in range(0, data_gen_test.n):\n","    _, y = data_gen_test.__getitem__(j)\n","    sub_total.append(y)\n","print(type(sub_total[0]))\n","Y_true = np.concatenate(sub_total, axis = 0)\n","print(Y_true.shape)\n"],"id":"267e8673","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["2498\n","<class 'numpy.ndarray'>\n","(249800, 500)\n"]}]},{"cell_type":"code","metadata":{"id":"6ffcd1c6"},"source":["print(Y_true)"],"id":"6ffcd1c6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcf20193","scrolled":true,"outputId":"4ab18aa5-4dd9-4026-d500-775fa47b840f"},"source":["\n","\n","from sklearn.metrics import confusion_matrix, average_precision_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","    \n","\n","Y_test_comp = Y_true.argmax(axis=1)\n","Y_predict_comp = Y_predict.argmax(axis=1)\n","Y_predict_comp = Y_predict_comp[0:len(Y_test_comp)]\n","\n","arr = confusion_matrix(Y_test_comp, Y_predict_comp)\n","print('Accuracy: {:.2f}'.format(accuracy_score(Y_test_comp, Y_predict_comp)))\n","print('Weighted Precision: {:.2f}'.format(precision_score(Y_test_comp, Y_predict_comp, average='weighted')))\n","print('Weighted Recall: {:.2f}'.format(recall_score(Y_test_comp, Y_predict_comp, average='weighted')))\n","print('Weighted F1-score: {:.2f}'.format(f1_score(Y_test_comp, Y_predict_comp, average='weighted')))\n","print(\"#############################################\")\n","\n"],"id":"bcf20193","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.07\n","Weighted Precision: 0.00\n","Weighted Recall: 0.07\n","Weighted F1-score: 0.01\n","#############################################\n"]},{"name":"stderr","output_type":"stream","text":["/home/vijay/anaconda3/envs/ayushenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72b29b61","outputId":"5a9cdfa4-3d50-48e4-d847-e3d23b191bf7"},"source":["tmp_y = df_test['Delta'].copy()\n","\n","df_test['PC'] = oe_PC.transform(df_test['PC'].values.reshape(-1, 1))\n","df_test['Delta'] = oe_Delta.transform(df_test['Delta'].values.reshape(-1, 1))\n","\n","testX = []\n","testY = []\n","n_past =20\n","n_future = 1\n","\n","for i in range(n_past, len(df_test) - n_future +1):\n","    testX.append(df_test.iloc[i - n_past:i, :])#include pc and delta\n","    testY.append(ohe.transform(np.array(tmp_y[i + n_future - 1:i + n_future]).reshape(-1, 1)))\n","testX = np.array(testX)\n","testY = np.array(testY)\n","\n"],"id":"72b29b61","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-11-fc71fa543719>:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_test['PC'] = oe_PC.transform(df_test['PC'].values.reshape(-1, 1))\n","<ipython-input-11-fc71fa543719>:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_test['Delta'] = oe_Delta.transform(df_test['Delta'].values.reshape(-1, 1))\n"]}]},{"cell_type":"code","metadata":{"id":"1393d43b"},"source":["shp = testY.shape\n","testY = testY.reshape(shp[0], shp[2])"],"id":"1393d43b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ae31a72","outputId":"900ec87c-30db-45c6-8d41-c1bbc645ee90"},"source":["saved_model = load_model('best_model.h5')\n","train_loss = saved_model.evaluate(np.split(testX, 2, axis=2), testY, verbose=1)\n","Y_predict = saved_model.predict(np.split(testX, 2, axis=2))\n","print(train_loss)"],"id":"4ae31a72","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","7812/7812 [==============================] - 123s 16ms/step - loss: 0.9413 - accuracy: 0.7287\n"]}]}]}